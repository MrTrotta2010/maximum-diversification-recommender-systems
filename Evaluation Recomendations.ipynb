{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T19:57:02.201167Z",
     "start_time": "2018-12-19T19:56:59.373648Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T19:57:02.321385Z",
     "start_time": "2018-12-19T19:57:02.317823Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_file_mymedialite(file_top_n, output_file):\n",
    "    # read the training data and mount the matrix\n",
    "    file_top_n = open(file_top_n, 'r')\n",
    "\n",
    "    file_out = open(output_file, 'w')\n",
    "\n",
    "    lines = []\n",
    "    ids = []\n",
    "    ratings = []\n",
    "    dic_u_top = {}\n",
    "    cont_u = 0\n",
    "    for line in file_top_n:\n",
    "        aux = line.split()\n",
    "        user = aux[0]\n",
    "        aux = aux[1][1:]\n",
    "        aux = aux[:-1].split(\",\")\n",
    "        file_out.write(\"%s %s\\n\" % (user, \" \".join(aux)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T19:57:02.464164Z",
     "start_time": "2018-12-19T19:57:02.459890Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_graphs(listX, listY, labelX, labelY, out='nothing', t='nothing'):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    plt.plot(listX, listY, linewidth=3.0)\n",
    "\n",
    "    ax.set_xlabel(labelX, fontsize='xx-large', labelpad=25, weight='semibold')\n",
    "    ax.set_ylabel(labelY, fontsize='xx-large', labelpad=25, weight='semibold')\n",
    "    \n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "    plt.tick_params(axis='both', labelsize=20, pad=25)\n",
    "\n",
    "    # for tick in ax.xaxis.get_ticklabels():\n",
    "    #    tick.set_fontsize('x-large')\n",
    "    #    tick.set_weight('bold')\n",
    "\n",
    "    # for tick in ax.yaxis.get_ticklabels():\n",
    "    #    tick.set_fontsize('x-large')\n",
    "    #    tick.set_weight('bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if t != 'nothing':\n",
    "        plt.title(t, fontsize='xx-large', weight='semibold')\n",
    "\n",
    "    #plt.savefig(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Dataset ML1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T19:56:52.862059Z",
     "start_time": "2018-12-19T19:56:52.444257Z"
    }
   },
   "outputs": [],
   "source": [
    "filein = open('Datasets/ML-1M/ratings_test.txt', 'r')\n",
    "\n",
    "u_rating_test = {}\n",
    "for line in filein:\n",
    "    line = line.strip()\n",
    "    try:\n",
    "        values = line.split(\"::\")\n",
    "        u_id = values[0]\n",
    "        i_id = values[1]\n",
    "        r = values[2]\n",
    "        \n",
    "        if not u_rating_test.get(u_id, False):\n",
    "            u_rating_test[u_id] = {}\n",
    "        u_rating_test[u_id][i_id] = r\n",
    "        \n",
    "    except:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Recommender List and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_list = ['rec_itemKNN', 'rec_userKNN', 'rec_WRMF']\n",
    "rec_n = ['', '_5', '_10', '_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_list = ['rec_WRMF']\n",
    "rec_n = ['_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r_list in rec_list:\n",
    "    for r_n in rec_n:\n",
    "        filein = open(\"Recommendations-Lists/ML-1M/\" +r_list+ \"\" +r_n+ \".txt\", 'r')\n",
    "\n",
    "        u_rating_pred = {}\n",
    "        for line in filein:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                values = line.split(\"\\t\")\n",
    "                u_id = values[0]\n",
    "                values = values[1][1:-1].split(\",\")\n",
    "\n",
    "                for i in values:\n",
    "                    v = i.split(\":\")\n",
    "                    i_id = v[0]\n",
    "                    r = v[1]\n",
    "\n",
    "                    if not u_rating_pred.get(u_id, False):\n",
    "                        u_rating_pred[u_id] = {}\n",
    "                    u_rating_pred[u_id][i_id] = r\n",
    "\n",
    "            except:\n",
    "                print(line)\n",
    "\n",
    "        ################\n",
    "        ### ACCURACY ###\n",
    "        ################\n",
    "\n",
    "        print(n)\n",
    "        print(r_list)\n",
    "        print(r_n)\n",
    "        fileout = open(\"Evaluations/Standard/\" +r_list+ \"/\" +r_list+ \"\" +r_n+ \"_acc.txt\", 'w')\n",
    "        fileout2 = open(\"Evaluations/Standard/\" +r_list+ \"/\" +r_list+ \"_eval.txt\", 'a')\n",
    "\n",
    "        u_acc = {}\n",
    "        acc_median = []\n",
    "        acc_average = []\n",
    "        for i,user in enumerate(u_rating_pred):\n",
    "            u_acc[user] = 0\n",
    "            for j, item in enumerate(u_rating_pred[user]):\n",
    "                if u_rating_test[user].get(item, False):\n",
    "                    u_acc[user] += 1\n",
    "\n",
    "            u_acc[user] /= len(u_rating_pred[user].keys())\n",
    "            acc_average.append(u_acc[user])\n",
    "            acc_median.append(u_acc[user])\n",
    "\n",
    "        fileout2.write(\"\\n\")\n",
    "        fileout2.write(\"%s%s\\n\" % (r_list,r_n))\n",
    "        fileout2.write(\"Accuracy Average: %f\\n\" % (np.mean(acc_average)))\n",
    "        fileout2.write(\"Accuracy Median: %f\\n\" % (np.median(acc_median)))\n",
    "        fileout2.write(\"-------------------------------\\n\")\n",
    "        for i, v in enumerate(u_acc):\n",
    "            fileout.write(\"%s\\t%f\\n\" % (v, u_acc[v]))\n",
    "\n",
    "        #plot\n",
    "        #listY = list(u_acc.values())\n",
    "        #listY.sort(reverse=True)\n",
    "        #print(listY[0])\n",
    "        #plot_graphs((np.arange(len(u_acc.keys())) / len(u_acc.keys())) * 100, listY, \"Users\", \"Acc\", out='nothing', t=rec_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convert_file_mymedialite(\"Recommendations-Lists/rec_itemKNN.txt\", \"Recommendations-Lists/rec_itemKNN_conv.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command = \"cd frameworks/vargas/src/ && make && ./getMetrics \" \\\n",
    "              \"-b ../../../Datasets/ML-1M/ratings_train.txt \" \\\n",
    "              \"-p ../../../Recommendations-Lists/\" +rec_list+ \"_conv.txt \" \\\n",
    "              \"-o ../../../Evaluations/Standard/\" +rec_list+ \"_div.txt \" \\\n",
    "              \"-l ../../../Datasets/ML-1M/ratings_test.txt \"\\\n",
    "              \"-n 100\"\\\n",
    "              \" && make clean\"\n",
    "print(command)\n",
    "subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cd frameworks/vargas/src/ && make && ./getMetrics -b ../../../Datasets/ML-1M/ratings_train.txt -p ../../../Recommendations-Lists/rec_MostPopular_conv.txt -o ../../../Evaluations/Standard/rec_MostPopular_div.txt -l ../../../Datasets/ML-1M/ratings_test.txt -n 5 -t 4 && make clean\n"
     ]
    }
   ],
   "source": [
    "#rec_list = ['rec_itemKNN_5', 'rec_itemKNN_10', 'rec_itemKNN_20',\n",
    "#                 'rec_MostPopular', 'rec_MostPopular_5', 'rec_MostPopular_10', 'rec_MostPopular_20',\n",
    "#                 'rec_userKNN', 'rec_userKNN_5', 'rec_userKNN_10', 'rec_userKNN_20',\n",
    "#                 'rec_WRMF', 'rec_WRMF_5', 'rec_WRMF_10', 'rec_WRMF_20']\n",
    "#rec_list = ['rec_MostPopular', 'rec_MostPopular_5', 'rec_MostPopular_10', 'rec_MostPopular_20']\n",
    "#rec_n = [5, 10, 20, 100, 5, 10, 20, 100, 5, 10, 20, 100, 5, 10, 20]\n",
    "rec_list = ['rec_MostPopular']\n",
    "rec_n = [5]\n",
    "for i in range(len(rec_list)):\n",
    "    convert_file_mymedialite(\"Recommendations-Lists/\"+rec_list[i]+\".txt\", \"Recommendations-Lists/\"+rec_list[i]+\"_conv.txt\")\n",
    "    command = \"cd frameworks/vargas/src/ && make && ./getMetrics \" \\\n",
    "              \"-b ../../../Datasets/ML-1M/ratings_train.txt \" \\\n",
    "              \"-p ../../../Recommendations-Lists/\" +rec_list[i]+ \"_conv.txt \" \\\n",
    "              \"-o ../../../Evaluations/Standard/\" +rec_list[i]+ \"_div.txt \" \\\n",
    "              \"-l ../../../Datasets/ML-1M/ratings_test.txt \"\\\n",
    "              \"-n \"+ str(rec_n[i]) +\" \"\\\n",
    "              \"-t 4\"\\\n",
    "              \" && make clean\"\n",
    "    print(command)\n",
    "    subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
